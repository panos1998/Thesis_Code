{"cells":[{"cell_type":"code","execution_count":1,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.fillna(-1)\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = max(scores, key=lambda score: score[0])\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [01:19<00:00, 12.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Maximum younden,specificity, sensitivity, threshold  (0.388885945967972, 0.4662055335968379, 0.9226804123711341, 0.9179179179179179)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 104.17it/s]"]},{"output_type":"stream","name":"stdout","text":["0.7806466729147141\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.fillna(-1)\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = max(scores, key=lambda score: score[0])\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [01:18<00:00, 12.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Maximum younden,specificity, sensitivity, threshold  (0.3811682490526058, 0.4492094861660079, 0.931958762886598, 0.5745745745745746)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 114.91it/s]"]},{"output_type":"stream","name":"stdout","text":["0.7774550751803104\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.dropna()\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = max(scores, key=lambda score: score[0])\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [01:07<00:00, 14.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Maximum younden,specificity, sensitivity, threshold  (0.6080376280066683, 0.6874493927125506, 0.9205882352941177, 0.26626626626626626)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 119.03it/s]"]},{"output_type":"stream","name":"stdout","text":["0.8732555370326269\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.dropna()\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = max(scores, key=lambda score: score[0])\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [01:14<00:00, 13.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Maximum younden,specificity, sensitivity, threshold  (0.608359133126935, 0.6789473684210526, 0.9294117647058824, 0.4724724724724725)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 121.96it/s]"]},{"output_type":"stream","name":"stdout","text":["0.866432483924744\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.dropna()\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=round(0.03*len(X)),\n","       train_size=round(0.07*len(X)) stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = max(scores, key=lambda score: score[0])\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax. Perhaps you forgot a comma? (<ipython-input-5-843ea1d860ac>, line 62)","traceback":["\u001b[1;36m  Input \u001b[1;32mIn [5]\u001b[1;36m\u001b[0m\n\u001b[1;33m    train_size=round(0.07*len(X)) stratify=y)\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"]}],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.dropna()\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=round(0.03*len(X)),\n","       train_size=round(0.07*len(X)), stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = max(scores, key=lambda score: score[0])\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [01:08<00:00, 14.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Maximum younden,specificity, sensitivity, threshold  (0.6666666666666667, 0.7, 0.9666666666666666, 0.9519519519519519)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 85.48it/s]"]},{"output_type":"stream","name":"stdout","text":["0.8642176708740177\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.dropna()\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=round(0.03*len(X)),\n","       train_size=round(0.07*len(X)), stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = max(scores, key=lambda score: score[0])\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y,\n","    test_size=round(0.03*len(X)), train_size=round(0.07*len(X)), stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [01:12<00:00, 13.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Maximum younden,specificity, sensitivity, threshold  (0.6373333333333333, 0.704, 0.9333333333333333, 0.5875875875875876)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 149.25it/s]"]},{"output_type":"stream","name":"stdout","text":["0.7519999999999999\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.dropna()\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=round(0.03*len(X)),\n","       train_size=round(0.07*len(X)), stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = max(scores, key=lambda score: score[0])\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y,\n","    test_size=round(0.03*len(X)), train_size=round(0.07*len(X)), stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [01:01<00:00, 16.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Maximum younden,specificity, sensitivity, threshold  (0.6520000000000001, 0.752, 0.9, 0.24024024024024024)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 142.86it/s]"]},{"output_type":"stream","name":"stdout","text":["0.804\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.fillna(-1)\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=round(0.03*len(X)),\n","       train_size=round(0.07*len(X)), stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = max(scores, key=lambda score: score[0])\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y,\n","    test_size=round(0.03*len(X)), train_size=round(0.07*len(X)), stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [01:12<00:00, 13.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Maximum younden,specificity, sensitivity, threshold  (0.39799999999999996, 0.47800000000000004, 0.9200000000000002, 0.9009009009009009)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 135.13it/s]"]},{"output_type":"stream","name":"stdout","text":["0.7515000000000001\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.fillna(-1)\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=round(0.03*len(X)),\n","       train_size=round(0.07*len(X)), stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = max(scores, key=lambda score: score[0])\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y,\n","    test_size=round(0.03*len(X)), train_size=round(0.07*len(X)), stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [01:13<00:00, 13.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Maximum younden,specificity, sensitivity, threshold  (0.39, 0.47000000000000003, 0.9200000000000002, 0.8018018018018018)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 124.96it/s]"]},{"output_type":"stream","name":"stdout","text":["0.708\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.fillna(-1)\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=round(0.03*len(X)),\n","       train_size=round(0.07*len(X)), stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = [max(scores, key=lambda score: score[0]), min(scores, key=lambda score: score[0])]\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y,\n","    test_size=round(0.03*len(X)), train_size=round(0.07*len(X)), stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:59<00:00, 16.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Maximum younden,specificity, sensitivity, threshold  [(0.378, 0.46799999999999997, 0.9099999999999999, 0.8228228228228228), (0.052000000000000005, 0.262, 0.7899999999999999, 0.23523523523523523)]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 142.86it/s]"]},{"output_type":"stream","name":"stdout","text":["0.7536\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":12,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.fillna(-1)\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=round(0.03*len(X)),\n","       train_size=round(0.07*len(X)), stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = [max(scores, key=lambda score: score[0]), mean(scores, key=lambda score: score[0])]\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y,\n","    test_size=round(0.03*len(X)), train_size=round(0.07*len(X)), stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [01:05<00:00, 15.22it/s]\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'mean' is not defined","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\panos\\Documents\\Διπλωματική\\code\\NBayes.py\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=68'>69</a>\u001b[0m       younden\u001b[39m.\u001b[39mappend([specificity\u001b[39m+\u001b[39msensitivity\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,specificity, sensitivity])\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=69'>70</a>\u001b[0m    scores\u001b[39m.\u001b[39mappend((\u001b[39msum\u001b[39m(you[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m you \u001b[39min\u001b[39;00m younden)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(younden),\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=70'>71</a>\u001b[0m    \u001b[39msum\u001b[39m(you[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m you \u001b[39min\u001b[39;00m younden)\u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(younden), \u001b[39m# per threshold\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=71'>72</a>\u001b[0m    \u001b[39msum\u001b[39m(you[\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m you \u001b[39min\u001b[39;00m younden)\u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(younden), thr))\n\u001b[1;32m---> <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=72'>73</a>\u001b[0m optimal \u001b[39m=\u001b[39m [\u001b[39mmax\u001b[39m(scores, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m score: score[\u001b[39m0\u001b[39m]), mean(scores, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m score: score[\u001b[39m0\u001b[39m])]\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=73'>74</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMaximum younden,specificity, sensitivity, threshold \u001b[39m\u001b[39m'\u001b[39m, optimal)\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=75'>76</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m10\u001b[39m)):\n","\u001b[1;31mNameError\u001b[0m: name 'mean' is not defined"]}],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.fillna(-1)\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=round(0.03*len(X)),\n","       train_size=round(0.07*len(X)), stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = [max(scores, key=lambda score: score[0]), np.mean(scores, key=lambda score: score[0])]\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y,\n","    test_size=round(0.03*len(X)), train_size=round(0.07*len(X)), stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [01:00<00:00, 16.52it/s]\n"]},{"output_type":"error","ename":"TypeError","evalue":"_mean_dispatcher() got an unexpected keyword argument 'key'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\panos\\Documents\\Διπλωματική\\code\\NBayes.py\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=68'>69</a>\u001b[0m       younden\u001b[39m.\u001b[39mappend([specificity\u001b[39m+\u001b[39msensitivity\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,specificity, sensitivity])\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=69'>70</a>\u001b[0m    scores\u001b[39m.\u001b[39mappend((\u001b[39msum\u001b[39m(you[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m you \u001b[39min\u001b[39;00m younden)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(younden),\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=70'>71</a>\u001b[0m    \u001b[39msum\u001b[39m(you[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m you \u001b[39min\u001b[39;00m younden)\u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(younden), \u001b[39m# per threshold\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=71'>72</a>\u001b[0m    \u001b[39msum\u001b[39m(you[\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m you \u001b[39min\u001b[39;00m younden)\u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(younden), thr))\n\u001b[1;32m---> <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=72'>73</a>\u001b[0m optimal \u001b[39m=\u001b[39m [\u001b[39mmax\u001b[39m(scores, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m score: score[\u001b[39m0\u001b[39m]), np\u001b[39m.\u001b[39;49mmean(scores, key\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m score: score[\u001b[39m0\u001b[39;49m])]\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=73'>74</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMaximum younden,specificity, sensitivity, threshold \u001b[39m\u001b[39m'\u001b[39m, optimal)\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=75'>76</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m10\u001b[39m)):\n","File \u001b[1;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n","\u001b[1;31mTypeError\u001b[0m: _mean_dispatcher() got an unexpected keyword argument 'key'"]}],"metadata":{}},{"cell_type":"code","execution_count":14,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","data = data.fillna(-1)\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=round(0.03*len(X)),\n","       train_size=round(0.07*len(X)), stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = [max(scores, key=lambda score: score[0]), np.mean(scores,axis=0)]\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y,\n","    test_size=round(0.03*len(X)), train_size=round(0.07*len(X)), stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [01:10<00:00, 14.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Maximum younden,specificity, sensitivity, threshold  [(0.376, 0.506, 0.8700000000000001, 0.92992992992993), array([0.220954, 0.366904, 0.85405 , 0.5     ])]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 131.58it/s]"]},{"output_type":"stream","name":"stdout","text":["0.7846\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","#print(pd.isna(data).sum())\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=round(0.03*len(X)),\n","       train_size=round(0.07*len(X)), stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = [max(scores, key=lambda score: score[0]), np.mean(scores,axis=0)]\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y,\n","    test_size=round(0.03*len(X)), train_size=round(0.07*len(X)), stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1000 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"ValueError","evalue":"Input contains NaN, infinity or a value too large for dtype('float64').","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\panos\\Documents\\Διπλωματική\\code\\NBayes.py\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=59'>60</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m10\u001b[39m):\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=60'>61</a>\u001b[0m    X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39mround\u001b[39m(\u001b[39m0.03\u001b[39m\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(X)),\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=61'>62</a>\u001b[0m     train_size\u001b[39m=\u001b[39m\u001b[39mround\u001b[39m(\u001b[39m0.07\u001b[39m\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(X)), stratify\u001b[39m=\u001b[39my)\n\u001b[1;32m---> <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=62'>63</a>\u001b[0m    clf\u001b[39m.\u001b[39;49mfit(X_train,y_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=63'>64</a>\u001b[0m    y_pred \u001b[39m=\u001b[39m(clf\u001b[39m.\u001b[39mpredict_proba(X_test)[:,\u001b[39m1\u001b[39m]\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.003\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mbool\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=64'>65</a>\u001b[0m    tn, fp, fn, tp \u001b[39m=\u001b[39m confusion_matrix(y_test, y_pred)\u001b[39m.\u001b[39mravel()\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:245\u001b[0m, in \u001b[0;36mGaussianNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=221'>222</a>\u001b[0m \u001b[39m\"\"\"Fit Gaussian Naive Bayes according to X, y.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=222'>223</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=223'>224</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=241'>242</a>\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=242'>243</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=243'>244</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(y\u001b[39m=\u001b[39my)\n\u001b[1;32m--> <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=244'>245</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_partial_fit(\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=245'>246</a>\u001b[0m     X, y, np\u001b[39m.\u001b[39;49munique(y), _refit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=246'>247</a>\u001b[0m )\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:402\u001b[0m, in \u001b[0;36mGaussianNB._partial_fit\u001b[1;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=398'>399</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=400'>401</a>\u001b[0m first_call \u001b[39m=\u001b[39m _check_partial_fit_first_call(\u001b[39mself\u001b[39m, classes)\n\u001b[1;32m--> <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=401'>402</a>\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, reset\u001b[39m=\u001b[39;49mfirst_call)\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=402'>403</a>\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=403'>404</a>\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/base.py?line=578'>579</a>\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/base.py?line=579'>580</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/base.py?line=580'>581</a>\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/base.py?line=581'>582</a>\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/base.py?line=583'>584</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=960'>961</a>\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=961'>962</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=963'>964</a>\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=964'>965</a>\u001b[0m     X,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=965'>966</a>\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=966'>967</a>\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=967'>968</a>\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=968'>969</a>\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=969'>970</a>\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=970'>971</a>\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=971'>972</a>\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=972'>973</a>\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=973'>974</a>\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=974'>975</a>\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=975'>976</a>\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=976'>977</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=978'>979</a>\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric)\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=980'>981</a>\u001b[0m check_consistent_length(X, y)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=793'>794</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=794'>795</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=795'>796</a>\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=796'>797</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=798'>799</a>\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=799'>800</a>\u001b[0m         _assert_all_finite(array, allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=801'>802</a>\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=802'>803</a>\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=106'>107</a>\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=107'>108</a>\u001b[0m         allow_nan\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=108'>109</a>\u001b[0m         \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many()\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=109'>110</a>\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=110'>111</a>\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=111'>112</a>\u001b[0m     ):\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=112'>113</a>\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=113'>114</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=114'>115</a>\u001b[0m             msg_err\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=115'>116</a>\u001b[0m                 type_err, msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=116'>117</a>\u001b[0m             )\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=117'>118</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=118'>119</a>\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=119'>120</a>\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n","\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."]}],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","print(data.mean)"],"outputs":[{"output_type":"stream","name":"stdout","text":["<bound method NDFrame._add_numeric_operations.<locals>.mean of       LeicGender  LeicRace  raeducl  mstat  shlt  hlthlm  mobilb  lgmusa  \\\n","0              0         0      2.0      3     2       0       0       0   \n","1              1         0      2.0      1     2       0       0       0   \n","2              0         0      3.0      5     3       0       0       1   \n","3              1         0      3.0      1     2       1       0       0   \n","4              0         0      2.0      1     2       0       1       0   \n","...          ...       ...      ...    ...   ...     ...     ...     ...   \n","2004           0         0      2.0      7     3       0       0       0   \n","2005           1         0      1.0      5     4       0       0       1   \n","2006           1         0      4.0      7     3       0       0       0   \n","2007           1         0      4.0      7     1       0       0       0   \n","2008           1         0      2.0      1     5       1       2       0   \n","\n","      grossa  finea  ...  ldl  trig   sys1  dias3  fglu  hba1c  hemda  \\\n","0          0      0  ...  3.5   1.2   98.0   57.0   5.3    5.6      0   \n","1          0      0  ...  3.2   0.8  117.0   73.0   NaN    5.3      0   \n","2          0      0  ...  3.7   0.8  131.0   76.0   4.8    5.7      1   \n","3          0      0  ...  4.1   2.7  141.0   79.0   4.3    5.4      0   \n","4          0      0  ...  2.8   1.0  140.0   77.0   4.8    5.1      0   \n","...      ...    ...  ...  ...   ...    ...    ...   ...    ...    ...   \n","2004       0      0  ...  2.9   1.7  156.0   91.0   NaN   43.0      1   \n","2005       0      0  ...  3.2   3.1  150.0   75.0   NaN   52.0      1   \n","2006       0      0  ...  2.3   1.0  132.0   67.0   NaN   50.0      0   \n","2007       0      0  ...  NaN   NaN  113.0   79.0   NaN    NaN      0   \n","2008       1      1  ...  1.9   1.1  165.0   91.0   NaN   58.0      1   \n","\n","      eatVegFru  everHighGlu  rYdiabe  \n","0             1            0        0  \n","1             1            0        0  \n","2             1            0        0  \n","3             1            0        0  \n","4             1            0        0  \n","...         ...          ...      ...  \n","2004          1            1        1  \n","2005          1            1        1  \n","2006          1            1        1  \n","2007          1            2        1  \n","2008          1            1        1  \n","\n","[2009 rows x 35 columns]>\n"]}],"metadata":{}},{"cell_type":"code","execution_count":17,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","print(data.mean())"],"outputs":[{"output_type":"stream","name":"stdout","text":["LeicGender         0.466401\n","LeicRace           0.018915\n","raeducl            2.016650\n","mstat              2.537083\n","shlt               2.657541\n","hlthlm             0.263813\n","mobilb             0.496267\n","lgmusa             0.833250\n","grossa             0.310602\n","finea              0.157790\n","LeicHBP            0.377800\n","LeicAge            0.921354\n","hearte             0.159283\n","psyche             0.099054\n","bmicat             3.194126\n","physActive         0.890991\n","drinkd_e           2.621970\n","smoken             0.126059\n","itot           27141.720466\n","cfoodo1m          53.909774\n","jphysa             3.693153\n","estwt             79.092185\n","wstva             97.015530\n","chol               5.806188\n","hdl                1.577166\n","ldl                3.462886\n","trig               1.685767\n","sys1             138.937500\n","dias3             87.895792\n","fglu               5.198584\n","hba1c             17.556384\n","hemda              0.300647\n","eatVegFru          0.982081\n","everHighGlu        0.659034\n","rYdiabe            0.160279\n","dtype: float64\n"]}],"metadata":{}},{"cell_type":"code","execution_count":18,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","print(data.describe())"],"outputs":[{"output_type":"stream","name":"stdout","text":["        LeicGender     LeicRace      raeducl        mstat         shlt  \\\n","count  2009.000000  2009.000000  1982.000000  2009.000000  2009.000000   \n","mean      0.466401     0.018915     2.016650     2.537083     2.657541   \n","std       0.498994     0.136258     1.066788     2.449234     1.078675   \n","min       0.000000     0.000000     0.000000     1.000000     1.000000   \n","25%       0.000000     0.000000     1.000000     1.000000     2.000000   \n","50%       0.000000     0.000000     2.000000     1.000000     3.000000   \n","75%       1.000000     0.000000     3.000000     5.000000     3.000000   \n","max       1.000000     1.000000     4.000000     8.000000     5.000000   \n","\n","            hlthlm       mobilb      lgmusa       grossa        finea  ...  \\\n","count  2009.000000  2009.000000  2009.00000  2009.000000  2009.000000  ...   \n","mean      0.263813     0.496267     0.83325     0.310602     0.157790  ...   \n","std       0.443063     0.888231     1.18950     0.843945     0.447982  ...   \n","min       0.000000     0.000000     0.00000     0.000000     0.000000  ...   \n","25%       0.000000     0.000000     0.00000     0.000000     0.000000  ...   \n","50%       0.000000     0.000000     0.00000     0.000000     0.000000  ...   \n","75%       1.000000     1.000000     1.00000     0.000000     0.000000  ...   \n","max       2.000000     4.000000     4.00000     5.000000     3.000000  ...   \n","\n","               ldl         trig         sys1        dias3         fglu  \\\n","count  1587.000000  1616.000000  2000.000000  1996.000000  1059.000000   \n","mean      3.462886     1.685767   138.937500    87.895792     5.198584   \n","std       1.027148     0.985477    60.842613   107.263413     1.204013   \n","min       0.700000     0.400000    81.000000    31.000000     3.600000   \n","25%       2.800000     1.000000   122.000000    68.000000     4.600000   \n","50%       3.500000     1.400000   134.000000    75.000000     5.000000   \n","75%       4.100000     2.100000   147.000000    83.000000     5.400000   \n","max       8.000000    10.800000   999.000000   999.000000    20.100000   \n","\n","             hba1c        hemda    eatVegFru  everHighGlu      rYdiabe  \n","count  1582.000000  2009.000000  2009.000000  2009.000000  2009.000000  \n","mean     17.556384     0.300647     0.982081     0.659034     0.160279  \n","std      17.089662     0.522601     0.132691     0.793223     0.366956  \n","min       3.900000     0.000000     0.000000     0.000000     0.000000  \n","25%       5.500000     0.000000     1.000000     0.000000     0.000000  \n","50%       5.950000     0.000000     1.000000     0.000000     0.000000  \n","75%      37.000000     1.000000     1.000000     1.000000     0.000000  \n","max      95.000000     2.000000     1.000000     2.000000     1.000000  \n","\n","[8 rows x 35 columns]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":19,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","print(data.groupby(['rYdiabe']).describe())"],"outputs":[{"output_type":"stream","name":"stdout","text":["        LeicGender                                              LeicRace  \\\n","             count      mean       std  min  25%  50%  75%  max    count   \n","rYdiabe                                                                    \n","0           1687.0  0.452282  0.497865  0.0  0.0  0.0  1.0  1.0   1687.0   \n","1            322.0  0.540373  0.499143  0.0  0.0  1.0  1.0  1.0    322.0   \n","\n","                   ... eatVegFru      everHighGlu                           \\\n","             mean  ...       75%  max       count      mean       std  min   \n","rYdiabe            ...                                                       \n","0        0.016005  ...       1.0  1.0      1687.0  0.558388  0.789912  0.0   \n","1        0.034161  ...       1.0  1.0       322.0  1.186335  0.571438  0.0   \n","\n","                             \n","         25%  50%  75%  max  \n","rYdiabe                      \n","0        0.0  0.0  1.0  2.0  \n","1        1.0  1.0  2.0  2.0  \n","\n","[2 rows x 272 columns]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":20,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","print(data.groupby(['rYdiabe']).describe())"],"outputs":[{"output_type":"stream","name":"stdout","text":["        LeicGender                                              LeicRace  \\\n","             count      mean       std  min  25%  50%  75%  max    count   \n","rYdiabe                                                                    \n","0           1687.0  0.452282  0.497865  0.0  0.0  0.0  1.0  1.0   1687.0   \n","1            322.0  0.540373  0.499143  0.0  0.0  1.0  1.0  1.0    322.0   \n","\n","                   ... eatVegFru      everHighGlu                           \\\n","             mean  ...       75%  max       count      mean       std  min   \n","rYdiabe            ...                                                       \n","0        0.016005  ...       1.0  1.0      1687.0  0.558388  0.789912  0.0   \n","1        0.034161  ...       1.0  1.0       322.0  1.186335  0.571438  0.0   \n","\n","                             \n","         25%  50%  75%  max  \n","rYdiabe                      \n","0        0.0  0.0  1.0  2.0  \n","1        1.0  1.0  2.0  2.0  \n","\n","[2 rows x 272 columns]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":21,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","print(data.groupby(['rYdiabe']).describe())"],"outputs":[{"output_type":"stream","name":"stdout","text":["        LeicGender                                              LeicRace  \\\n","             count      mean       std  min  25%  50%  75%  max    count   \n","rYdiabe                                                                    \n","0           1687.0  0.452282  0.497865  0.0  0.0  0.0  1.0  1.0   1687.0   \n","1            322.0  0.540373  0.499143  0.0  0.0  1.0  1.0  1.0    322.0   \n","\n","                   ... eatVegFru      everHighGlu                           \\\n","             mean  ...       75%  max       count      mean       std  min   \n","rYdiabe            ...                                                       \n","0        0.016005  ...       1.0  1.0      1687.0  0.558388  0.789912  0.0   \n","1        0.034161  ...       1.0  1.0       322.0  1.186335  0.571438  0.0   \n","\n","                             \n","         25%  50%  75%  max  \n","rYdiabe                      \n","0        0.0  0.0  1.0  2.0  \n","1        1.0  1.0  2.0  2.0  \n","\n","[2 rows x 272 columns]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":22,"source":["#print(pd.isna(data).sum())\n","X = data[all_labels[:-1]] # get the features\n","y= data[all_labels[len(all_labels)-1]]#data[all_labels[len(all_labels)-1]] # get the target class\n","#from mixed_naive_bayes import MixedNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","categorical_features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,32,33,34]\n","clf = GaussianNB()\n","roc = list()\n","scores = list()\n","thresholds = np.linspace(0, 1, 1000)\n","for thr in tqdm.tqdm(thresholds):\n","   younden = list()\n","   for i in range(0,10):\n","      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=round(0.03*len(X)),\n","       train_size=round(0.07*len(X)), stratify=y)\n","      clf.fit(X_train,y_train)\n","      y_pred =(clf.predict_proba(X_test)[:,1]>=0.003).astype(bool)\n","      tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","      specificity = tn/(tn+fp)\n","      sensitivity = tp/(tp+fn)\n","      younden.append([specificity+sensitivity-1,specificity, sensitivity])\n","   scores.append((sum(you[0] for you in younden)/len(younden),\n","   sum(you[1] for you in younden)/ len(younden), # per threshold\n","   sum(you[2] for you in younden)/ len(younden), thr))\n","optimal = [max(scores, key=lambda score: score[0]), np.mean(scores,axis=0)]\n","print('Maximum younden,specificity, sensitivity, threshold ', optimal)\n","\n","for i in tqdm.tqdm(range(0,10)):\n","   X_train, X_test, y_train, y_test = train_test_split(X, y,\n","    test_size=round(0.03*len(X)), train_size=round(0.07*len(X)), stratify=y)\n","   clf.fit(X_train,y_train)\n","   y_pred =clf.predict_proba(X_test)[:,1]\n","   roc.append(roc_auc_score(y_test, y_pred))\n","print(np.mean(roc))\n","#clf = MixedNB(categorical_features=categorical_features)\n","#X.to_csv('NaiveBayes.csv',header=False, index=False)\n","#X = pd.read_csv('NaiveBayes.csv', header=None, index_col=False)\n","#print(pd.isna(X).sum())\n","#print(X.head(1371))\n","#clf.predict(X)\n","#print(X)\n","# Apply machine learning techniques"],"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1000 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"ValueError","evalue":"Input contains NaN, infinity or a value too large for dtype('float64').","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\panos\\Documents\\Διπλωματική\\code\\NBayes.py\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=61'>62</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m10\u001b[39m):\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=62'>63</a>\u001b[0m    X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39mround\u001b[39m(\u001b[39m0.03\u001b[39m\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(X)),\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=63'>64</a>\u001b[0m     train_size\u001b[39m=\u001b[39m\u001b[39mround\u001b[39m(\u001b[39m0.07\u001b[39m\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(X)), stratify\u001b[39m=\u001b[39my)\n\u001b[1;32m---> <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=64'>65</a>\u001b[0m    clf\u001b[39m.\u001b[39;49mfit(X_train,y_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=65'>66</a>\u001b[0m    y_pred \u001b[39m=\u001b[39m(clf\u001b[39m.\u001b[39mpredict_proba(X_test)[:,\u001b[39m1\u001b[39m]\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.003\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mbool\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/panos/Documents/%CE%94%CE%B9%CF%80%CE%BB%CF%89%CE%BC%CE%B1%CF%84%CE%B9%CE%BA%CE%AE/code/NBayes.py?line=66'>67</a>\u001b[0m    tn, fp, fn, tp \u001b[39m=\u001b[39m confusion_matrix(y_test, y_pred)\u001b[39m.\u001b[39mravel()\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:245\u001b[0m, in \u001b[0;36mGaussianNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=221'>222</a>\u001b[0m \u001b[39m\"\"\"Fit Gaussian Naive Bayes according to X, y.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=222'>223</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=223'>224</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=241'>242</a>\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=242'>243</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=243'>244</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(y\u001b[39m=\u001b[39my)\n\u001b[1;32m--> <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=244'>245</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_partial_fit(\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=245'>246</a>\u001b[0m     X, y, np\u001b[39m.\u001b[39;49munique(y), _refit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=246'>247</a>\u001b[0m )\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:402\u001b[0m, in \u001b[0;36mGaussianNB._partial_fit\u001b[1;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=398'>399</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=400'>401</a>\u001b[0m first_call \u001b[39m=\u001b[39m _check_partial_fit_first_call(\u001b[39mself\u001b[39m, classes)\n\u001b[1;32m--> <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=401'>402</a>\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, reset\u001b[39m=\u001b[39;49mfirst_call)\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=402'>403</a>\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/naive_bayes.py?line=403'>404</a>\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/base.py?line=578'>579</a>\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/base.py?line=579'>580</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/base.py?line=580'>581</a>\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/base.py?line=581'>582</a>\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/base.py?line=583'>584</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=960'>961</a>\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=961'>962</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=963'>964</a>\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=964'>965</a>\u001b[0m     X,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=965'>966</a>\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=966'>967</a>\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=967'>968</a>\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=968'>969</a>\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=969'>970</a>\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=970'>971</a>\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=971'>972</a>\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=972'>973</a>\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=973'>974</a>\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=974'>975</a>\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=975'>976</a>\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=976'>977</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=978'>979</a>\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric)\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=980'>981</a>\u001b[0m check_consistent_length(X, y)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=793'>794</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=794'>795</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=795'>796</a>\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=796'>797</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=798'>799</a>\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=799'>800</a>\u001b[0m         _assert_all_finite(array, allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=801'>802</a>\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=802'>803</a>\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=106'>107</a>\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=107'>108</a>\u001b[0m         allow_nan\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=108'>109</a>\u001b[0m         \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many()\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=109'>110</a>\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=110'>111</a>\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=111'>112</a>\u001b[0m     ):\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=112'>113</a>\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=113'>114</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=114'>115</a>\u001b[0m             msg_err\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=115'>116</a>\u001b[0m                 type_err, msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=116'>117</a>\u001b[0m             )\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=117'>118</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=118'>119</a>\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panos/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/utils/validation.py?line=119'>120</a>\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n","\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."]}],"metadata":{}},{"cell_type":"code","execution_count":23,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","print(data.groupby(['rYdiabe']).describe())"],"outputs":[{"output_type":"stream","name":"stdout","text":["        LeicGender                                              LeicRace  \\\n","             count      mean       std  min  25%  50%  75%  max    count   \n","rYdiabe                                                                    \n","0           1687.0  0.452282  0.497865  0.0  0.0  0.0  1.0  1.0   1687.0   \n","1            322.0  0.540373  0.499143  0.0  0.0  1.0  1.0  1.0    322.0   \n","\n","                   ... eatVegFru      everHighGlu                           \\\n","             mean  ...       75%  max       count      mean       std  min   \n","rYdiabe            ...                                                       \n","0        0.016005  ...       1.0  1.0      1687.0  0.558388  0.789912  0.0   \n","1        0.034161  ...       1.0  1.0       322.0  1.186335  0.571438  0.0   \n","\n","                             \n","         25%  50%  75%  max  \n","rYdiabe                      \n","0        0.0  0.0  1.0  2.0  \n","1        1.0  1.0  2.0  2.0  \n","\n","[2 rows x 272 columns]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":24,"source":["from turtle import width\n","from arfftocsv import processing\n","import numpy as np\n","import pandas as pd\n","import tqdm\n","\n","all_labels = ['LeicGender','LeicRace','raeducl','mstat','shlt','hlthlm',\n","'mobilb','lgmusa','grossa','finea','LeicHBP','LeicAge','hearte',\n","'psyche','bmicat','physActive','drinkd_e','smoken','itot','cfoodo1m',\n","'jphysa','estwt','wstva','chol','hdl','ldl','trig','sys1','dias3',\n","'fglu','hba1c','hemda','eatVegFru','everHighGlu','rYdiabe']\n","# this function deletes @ and empty lines so that produce a \n","\n","to_replace = {'LeicAge': ['50-59', '60-69', '>=70'], 'LeicGender': ['Female', 'Male'], \n","'bmicat': [\"'1.underweight less than 18.5'\",\n"," \"'2.normal weight from 18.5 to 24.9'\", \"'3.pre-obesity from 25 to 29.9'\",\n"," \"'4.obesity class 1 from 30 to 34.9'\", \"'5.obesity class 2 from 35 to 39.9'\",\n","  \"'6.obesity class 3 greater than 40'\"],\n","'LeicRace': [0, 6], 'hemda': [\"'Not applicable'\", 'Yes', 'No'],\n"," 'wstva':[],'LeicHBP': ['No', 'Yes'], 'rYdiabe': ['0.no', '1.yes'],\n"," 'raeducl':[\"'2.upper secondary and vocational training'\",'3.tertiary',\n"," \"'1.less than secondary'\",'.o:other',\"'.h:missing HSE value'\",\".m:Missing\"],\n","  'mstat':['3.partnered','1.married','5.divorced','7.widowed','4.separated',\"'8.never married'\"],\n","  'shlt':[\"'2.Very good'\",'3.Good','1.Excellent','4.Fair','5.Poor'], 'hlthlm':['0.no','1.yes','.d:DK'],\n","  'hearte':['0.no','1.yes'],'psyche':['0.no','1.yes'], 'physActive':['Yes','No'], 'smoken':['0.No',\n","  '1.Yes',\".m:Missing\"],\n","   'jphysa': [\"'1.Sedentary occupation'\",\"'3.Physical work'\",\"'2.Standing occupation'\",\"'.w:not working'\",\n","   \"'4.Heavy manual work'\",'.m:Missing'], 'everHighGlu':['No',\"'Not applicable'\",'Yes'],\n","   'eatVegFru':['Yes', 'No'],'mobilb':[],'lgmusa':[], 'grossa':[],'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':\n","   [], 'estwt':[], 'chol':[],'ldl':[], 'hdl':[], 'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n","}\n","\n","values = {'LeicAge': [0, 1, 2], 'LeicGender': [0, 1],\n"," 'bmicat': [1, 2, 3, 4, 5, 6],'LeicRace': [0, 1],\n"," 'hemda': [0, 1, 2], 'wstva':[],'LeicHBP':[0,1],'rYdiabe': [0, 1], 'raeducl':[2,3,1,0,4,np.nan], 'mstat':[\n","     3,1,5,7,4,8],'shlt':[2,3,1,4,5], 'hlthlm':[0,1,2],'hearte':[0,1], 'psyche':[0,1], 'physActive':[1,0],\n","  'smoken':[0,1,np.nan], 'jphysa':[1,3,2,5,4,np.nan], 'everHighGlu':[0,2,1], 'eatVegFru':[1,0],'mobilb':[],'lgmusa':[],\n","  'grossa':[], 'finea':[], 'drinkd_e':[],'itot':[],'cfoodo1m':[], 'estwt':[],'chol':[],'ldl':[], 'hdl':[],\n","  'trig':[], 'sys1':[], 'dias3':[], 'fglu':[], 'hba1c':[]\n"," }\n","\n","path = 'NBayes.csv'\n","data = processing(labels=all_labels, to_replace=to_replace,all_labels=all_labels, values= values, path=path)\n","print(data.groupby(['rYdiabe']).describe())"],"outputs":[{"output_type":"stream","name":"stdout","text":["        LeicGender                                              LeicRace  \\\n","             count      mean       std  min  25%  50%  75%  max    count   \n","rYdiabe                                                                    \n","0           1687.0  0.452282  0.497865  0.0  0.0  0.0  1.0  1.0   1687.0   \n","1            322.0  0.540373  0.499143  0.0  0.0  1.0  1.0  1.0    322.0   \n","\n","                   ... eatVegFru      everHighGlu                           \\\n","             mean  ...       75%  max       count      mean       std  min   \n","rYdiabe            ...                                                       \n","0        0.016005  ...       1.0  1.0      1687.0  0.558388  0.789912  0.0   \n","1        0.034161  ...       1.0  1.0       322.0  1.186335  0.571438  0.0   \n","\n","                             \n","         25%  50%  75%  max  \n","rYdiabe                      \n","0        0.0  0.0  1.0  2.0  \n","1        1.0  1.0  2.0  2.0  \n","\n","[2 rows x 272 columns]\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}